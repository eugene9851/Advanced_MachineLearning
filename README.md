# MachineLearning Lecture (2024.04 ~ 2024.05)

tools: python, sklearn, matplotlib, colab
Data: iris, makemoons, titanic, wine, Kaggle Credit Score Classification 

1. Statistics
2. Data preprocessing
   1) MinMax, Stadard, Robust Scaler
   2) train valid test set split
4. Linear Regression
5. Regularization - Ridge, Lasso, Elastic Net
6. Logistic Regression
7. Support Vector machine
8. Decision Tree, Graphviz
9. Random Forest
10. Ensemble functions (Bagging, Voting Classifier)
11. Gradient Boost, AdaBoost, XGboost, LightGBM, Catboost
12. Stacking
13. AutoML - pycaret, autogluon
14. Hyperparameter tuning - bayesian optimization, optuna


References
1. Chen, Tianqi, and Carlos Guestrin. "Xgboost: A scalable tree boosting system." Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016.
2. Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. "Practical bayesian optimization of machine learning algorithms." Advances in neural information processing systems 25 (2012).
3. Aurelien, Geron, "Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow", 2019
